{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5nXgMETmwrvtvkR/XDP67",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgh4000/NLPSafeAI/blob/sh%2Finitial/Lab1and2_newdataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqesmRA7R2dH"
      },
      "outputs": [],
      "source": [
        "# Using this dataset - https://www.kaggle.com/datasets/infamouscoder/depression-reddit-cleaned?resource=download\n",
        "\n",
        "# Following the Kaggle download instructions\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "import pandas as pd\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"depression_dataset_reddit_cleaned.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"infamouscoder/depression-reddit-cleaned\",\n",
        "  file_path,\n",
        "  # Provide any additional arguments like\n",
        "  # sql_query or pandas_kwargs. See the\n",
        "  # documenation for more information:\n",
        "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        ")\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "clean_text = df['clean_text']\n",
        "display(clean_text[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigate the data\n",
        "\n",
        "# Count how many records of each type, depression means is_depression = 1 and no depression means is_depression = 0\n",
        "\n",
        "depression_count = df['is_depression'].value_counts()\n",
        "print(depression_count)"
      ],
      "metadata": {
        "id": "LugaXedEU-mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "### Import the relevant libraries ###\n",
        "#####################################\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "HNtCfAWpV9HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train/test\n",
        "\n",
        "df_train, df_test = train_test_split(\n",
        "    df, test_size=0.2, random_state=42, stratify=df[\"is_depression\"]\n",
        ")\n",
        "\n",
        "# Extract training features and labels if needed. Keeping raw for future analysis\n",
        "X_train_raw = df_train[\"clean_text\"].tolist()\n",
        "y_train = df_train[\"is_depression\"].values\n",
        "X_test_raw = df_test[\"clean_text\"].tolist()\n",
        "y_test = df_test[\"is_depression\"].values"
      ],
      "metadata": {
        "id": "4-LPIiFLVfQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "### I want to do some analysis on the PCA embeddings ###\n",
        "########################################################\n",
        "\n",
        "# Also, I want to investigate how different number of components has an effect\n",
        "\n",
        "# The pca.explained_variance_ratio returns a vector of the variance explained by each dimension\n",
        "# So [0.11,0.095,0.085] means that the first dimension explains 11% of variance and so on\n",
        "# explained_variance_ratio_.cumsum gives the cumulated totals, so taking the final value gives the\n",
        "# total variance explained by all dimensions\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "encoding_model = \"all-MiniLM-L6-v2\"\n",
        "encoder = SentenceTransformer(encoding_model)\n",
        "n_components = 30\n",
        "\n",
        "X_train = encoder.encode(X_train_raw, show_progress_bar=False)\n",
        "X_test = encoder.encode(X_test_raw, show_progress_bar=False)\n",
        "\n",
        "data = np.vstack([X_train])\n",
        "# PCA data\n",
        "data_pca = PCA(n_components=n_components).fit(data)\n",
        "\n",
        "print(\"---- n = 30 ----\")\n",
        "\n",
        "cum_sum_30 = data_pca.explained_variance_ratio_.cumsum()[-1]\n",
        "\n",
        "print(f\"Cum sum {n_components} is {cum_sum_30:.3f}\")\n",
        "\n",
        "n_components = 50\n",
        "\n",
        "X_train = encoder.encode(X_train_raw, show_progress_bar=False)\n",
        "X_test = encoder.encode(X_test_raw, show_progress_bar=False)\n",
        "\n",
        "data = np.vstack([X_train])\n",
        "# PCA data\n",
        "data_pca = PCA(n_components=n_components).fit(data)\n",
        "\n",
        "print(\"---- n = 50 ----\")\n",
        "\n",
        "cum_sum_50 = data_pca.explained_variance_ratio_.cumsum()[-1]\n",
        "\n",
        "print(f\"Cum sum {n_components} is {cum_sum_50:.3f}\")\n",
        "\n",
        "\n",
        "# n_components = 70\n",
        "\n",
        "# X_train = encoder.encode(X_train_raw, show_progress_bar=False)\n",
        "# X_test = encoder.encode(X_test_raw, show_progress_bar=False)\n",
        "\n",
        "# data = np.vstack([X_train])\n",
        "# PCA data\n",
        "# data_pca = PCA(n_components=n_components).fit(data)\n",
        "\n",
        "# print(\"---- n = 70 ----\")\n",
        "\n",
        "# cum_sum_70 = data_pca.explained_variance_ratio_.cumsum()[-1]\n",
        "\n",
        "# print(f\"Cum sum {n_components} is {cum_sum_70:.3f}\")\n",
        "\n",
        "# n_components = 100\n",
        "\n",
        "# X_train = encoder.encode(X_train_raw, show_progress_bar=False)\n",
        "# X_test = encoder.encode(X_test_raw, show_progress_bar=False)\n",
        "\n",
        "# data = np.vstack([X_train])\n",
        "# PCA data\n",
        "# data_pca = PCA(n_components=n_components).fit(data)\n",
        "\n",
        "# print(\"---- n = 100 ----\")\n",
        "\n",
        "# cum_sum_100 = data_pca.explained_variance_ratio_.cumsum()[-1]\n",
        "\n",
        "# print(f\"Cum sum {n_components} is {cum_sum_100:.3f}\")\n",
        "\n",
        "# n_components = 200\n",
        "\n",
        "# X_train = encoder.encode(X_train_raw, show_progress_bar=False)\n",
        "# X_test = encoder.encode(X_test_raw, show_progress_bar=False)\n",
        "\n",
        "# data = np.vstack([X_train])\n",
        "# PCA data\n",
        "# data_pca = PCA(n_components=n_components).fit(data)\n",
        "\n",
        "# print(\"---- n = 200 ----\")\n",
        "\n",
        "# cum_sum_200 = data_pca.explained_variance_ratio_.cumsum()[-1]\n",
        "\n",
        "# print(f\"Cum sum {n_components} is {cum_sum_200:.3f}\")\n",
        "\n",
        "# plt.plot([30, 50, 70, 100, 200], [cum_sum_30, cum_sum_50, cum_sum_70, cum_sum_100, cum_sum_200], marker='o')\n",
        "# plt.xlabel('Number of Principal Components')\n",
        "# plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "# plt.title('Cumulative Explained Variance Ratio by Principal Components')\n",
        "# plt.ylim(bottom=0)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R1qNpnvPWcW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################\n",
        "### Do the text embeddings needed for training ###\n",
        "##################################################\n",
        "\n",
        "encoding_model = \"all-MiniLM-L6-v2\"\n",
        "encoder = SentenceTransformer(encoding_model)\n",
        "n_components = 30\n",
        "batch_size = 64\n",
        "\n",
        "X_train = encoder.encode(X_train_raw, show_progress_bar=False)\n",
        "X_test = encoder.encode(X_test_raw, show_progress_bar=False)\n",
        "\n",
        "data = np.vstack([X_train])\n",
        "# PCA data\n",
        "data_pca = PCA(n_components=n_components).fit(data)\n",
        "\n",
        "X_train = data_pca.transform(X_train)\n",
        "X_test = data_pca.transform(X_test)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "6w0aiGPDdjEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################################################\n",
        "### Define a simple model - credit for this spec https://github.com/Tgl70/DAIR-course-NLP/blob/main/main.py ###\n",
        "###############################################################################################################\n",
        "\n",
        "print(y_train.shape, y_train[:10])\n",
        "print(y_test.shape, y_test[:10])\n",
        "\n",
        "def get_model(input_size):\n",
        "    initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
        "    model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Input(shape=(input_size,), name='input_features'),\n",
        "            tf.keras.layers.Dense(128, activation='relu', kernel_initializer=initializer, name='dense_1'),\n",
        "            tf.keras.layers.Dense(2, activation='softmax', kernel_initializer=initializer, name='output_layer')\n",
        "        ])\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "metadata": {
        "id": "tViZ0LRvdnyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "### Set some variables ###\n",
        "##########################\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "batch_size = 64\n",
        "epochs = 6\n",
        "n_classes = 2\n",
        "epsilon = 0.3\n",
        "alpha = 0.1\n",
        "num_iter = 10"
      ],
      "metadata": {
        "id": "U-lqFhvPdvyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################\n",
        "### Train the base model (simple and no adversarial) ###\n",
        "########################################################\n",
        "\n",
        "model_base = get_model(input_size)\n",
        "\n",
        "model_base.summary()\n",
        "\n",
        "model_base.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n",
        "    )\n",
        "\n",
        "model_base.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=epochs,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "MGewbooSdw6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################\n",
        "### Define the precision, recall, F1 and ROC curve function ###\n",
        "###############################################################\n",
        "\n",
        "# Taken from lab https://github.com/KatyaKom/DAIR/blob/main/Lab2/CV.ipynb\n",
        "\n",
        "def print_metrics(model, x, y, c):\n",
        "    # Get predicted probabilities for all classes\n",
        "    y_pred_prob = model.predict(x)\n",
        "\n",
        "    # Get predicted class labels (highest probability class)\n",
        "    y_pred_class = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score (using macro average)\n",
        "    precision = precision_score(y, y_pred_class, average='macro')\n",
        "    recall = recall_score(y, y_pred_class, average='macro')\n",
        "    f1 = f1_score(y, y_pred_class, average='macro')\n",
        "\n",
        "    # Display the macro/micro/weighted average metrics\n",
        "    print(f'Precision (macro): {precision:.4f}')\n",
        "    print(f'Recall (macro): {recall:.4f}')\n",
        "    print(f'F1-score (macro): {f1:.4f}')\n",
        "\n",
        "    y_test_bin = to_categorical(y, num_classes=c)\n",
        "\n",
        "    # Compute ROC curve and AUC for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(c):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_prob[:, i])\n",
        "        roc_auc[i] = roc_auc_score(y_test_bin[:, i], y_pred_prob[:, i])\n",
        "\n",
        "    # Plot the ROC curve for each class\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    for i in range(c):\n",
        "        plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')  # Dashed diagonal line\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) for Each Class')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "print_metrics(model_base, X_test, y_test, 2)"
      ],
      "metadata": {
        "id": "lNbfjnDhd3LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "### Calculate the loss and accuracy ###\n",
        "#######################################\n",
        "\n",
        "loss, acc = model_base.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"Base model loss: {loss:.3f}\")\n",
        "print(f\"Base model accuracy: {acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "JTzaW6ZAd6Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "### LIME Setup ###\n",
        "##################\n",
        "\n",
        "%pip install Lime\n",
        "from lime.lime_text import LimeTextExplainer"
      ],
      "metadata": {
        "id": "4UhWQuLad7Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "### LIME Explain ###\n",
        "####################\n",
        "\n",
        "# Local Interpretable Model-Agnostic Explanations\n",
        "# LIME is representing local explanation, need to be mindful that a local explanation isn't a global explanation\n",
        "\n",
        "queries = X_test_raw[:2]\n",
        "\n",
        "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "class_names = [\"non-depression\", \"depression\"]\n",
        "\n",
        "\n",
        "def make_predict_fn(model, encoder, pca):\n",
        "    def predict_fn(texts):\n",
        "        X_encoded = encoder.encode(texts, show_progress_bar=False)\n",
        "        X_pca = pca.transform(X_encoded)\n",
        "        preds = model.predict(X_pca)\n",
        "        return tf.nn.softmax(preds, axis=1).numpy()\n",
        "    return predict_fn\n",
        "\n",
        "def explain_text(query, model, encoder, pca, class_names, print_out=True):\n",
        "\n",
        "    print(f\"query: {query}\")\n",
        "\n",
        "    predict_fn = make_predict_fn(model, encoder, pca)\n",
        "    explainer = LimeTextExplainer(class_names=class_names, random_state=42)\n",
        "\n",
        "    print(predict_fn)\n",
        "\n",
        "    predicted_probs = predict_fn([query])\n",
        "    pred_class = int(np.argmax(predicted_probs))\n",
        "\n",
        "    exp = explainer.explain_instance(\n",
        "        query,\n",
        "        predict_fn,\n",
        "        num_features=10,\n",
        "        labels=[0,1]\n",
        "    )\n",
        "\n",
        "    print(\"Model probabilities:\", predicted_probs)\n",
        "\n",
        "    print(f\"Pred class: {pred_class}\")\n",
        "    if (print_out == True):\n",
        "      print(f\"\\nLIME Explanation for: '{query}'\")\n",
        "      available_labels = exp.available_labels()\n",
        "      for label in available_labels:\n",
        "          print(f\"\\nClass {label} ({class_names[label]}) explanation:\")\n",
        "          for word, weight in exp.as_list(label=label):\n",
        "              print(f\"  {word}: {weight:.4f}\")\n",
        "\n",
        "      print(exp.as_list(label=pred_class))\n",
        "\n",
        "      exp.as_pyplot_figure(label=pred_class)\n",
        "      plt.show()\n",
        "\n",
        "      print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "    return exp\n",
        "\n",
        "query_and_important_word = {}\n",
        "\n",
        "for q in queries:\n",
        "  exp = explain_text(q, model_base, encoder, data_pca, class_names)\n",
        "  query_and_important_word[q] = str(exp.as_list()[0][0])\n",
        "\n",
        "\n",
        "print(query_and_important_word)"
      ],
      "metadata": {
        "id": "7srDUTUrlMUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################\n",
        "### SHAP Analysis helper functions ###\n",
        "######################################\n",
        "\n",
        "import shap\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pk\n",
        "from sklearn.decomposition import PCA\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "def feature_importance_graph(X_train, X_test, model):\n",
        "  # Take a small subset of the training data to explain using SHAP\n",
        "  background = X_train[np.random.choice(X_train.shape[0], 100, replace=False)]\n",
        "  to_explain = X_test[:100]\n",
        "\n",
        "  explainer = shap.KernelExplainer(model.predict, background)\n",
        "  shap_values = explainer.shap_values(to_explain)\n",
        "\n",
        "  # Want to measure how features push towards class 1\n",
        "  shap_diff = shap_values[..., 1] - shap_values[..., 0]\n",
        "\n",
        "  shap.summary_plot(\n",
        "    shap_diff, to_explain,\n",
        "    feature_names=[f\"f{i}\" for i in range(X_train.shape[1])]\n",
        "  )\n",
        "\n",
        "  return shap_diff\n",
        "\n",
        "def get_associated_queries_for_top_features(no_top_PCA_features, no_top_embedding_dimensions, no_top_text_queries, pca, encoder, queries_raw, shap_diff):\n",
        "  # The idea here is that we can use SHAP to find the most important PCA features. From here, find the embeddings that drive them. Then look at real queries (from training data) that score highly for these embeddings.\n",
        "\n",
        "  encoder = SentenceTransformer(encoder)\n",
        "  embeddings = encoder.encode(queries_raw, show_progress_bar=False)\n",
        "\n",
        "  X_pca = pca.transform(embeddings)\n",
        "\n",
        "  # Get the mean absolute SHAP values per PCA feature\n",
        "  mean_abs_shap = np.abs(shap_diff).mean(axis=0)\n",
        "\n",
        "  # Find the top PCA features\n",
        "  sorted_indices = np.argsort(mean_abs_shap)\n",
        "  sorted_indices_desc = np.flip(sorted_indices)\n",
        "  top_features = sorted_indices_desc[:no_top_PCA_features]\n",
        "\n",
        "  print(f\"\\nTop {no_top_PCA_features} PCA features ranked by SHAP importance:\\n\")\n",
        "\n",
        "  # Loop over the identified top PCA features\n",
        "  for rank, feature in enumerate(top_features, start=1):\n",
        "    print(f\"{rank}. Feature {feature}\")\n",
        "    print(f\"  Mean absolute SHAP value: {mean_abs_shap[feature]}\")\n",
        "\n",
        "    # Find which embedding matters for the feature\n",
        "    component = pca.components_[feature]\n",
        "    abs_component = np.abs(component)\n",
        "    sorted_indices = np.argsort(abs_component)\n",
        "    sorted_indices_desc = np.flip(sorted_indices)\n",
        "    top_dimensions = sorted_indices_desc[:no_top_embedding_dimensions]\n",
        "\n",
        "    for dimension in top_dimensions:\n",
        "      weight = component[dimension]\n",
        "      print(f\"\\n  Embedding dimension {dimension} contributes to PCA feature f{feature} \"f\"({weight:+.4f})\")\n",
        "\n",
        "      # Identify queries where this embedding dimension is strong\n",
        "      # Take all sentences, and get this embedding dimension value\n",
        "      dimension_values = embeddings[:, dimension]\n",
        "      # Depends on sign, if positive then largest embedding value is making PCA feature important, if negative than smallest value\n",
        "      order = np.argsort(dimension_values)\n",
        "      if weight > 0:\n",
        "        # Then need to flip for descending\n",
        "        order = order[::-1]\n",
        "      # Get top queries\n",
        "      top_query = order[:no_top_text_queries]\n",
        "      listWords = []\n",
        "      for i in top_query:\n",
        "        query = queries_raw[i]\n",
        "        print(f\"    Query: {query} | dimension {dimension} value = {dimension_values[i]:.4f}\")\n",
        "        listWords.extend(query.split())\n",
        "\n",
        "      counts = Counter(listWords)\n",
        "      print(f\"Common words among top queries: {counts.most_common(5)}\")\n",
        "    print()\n",
        "\n",
        "  return mean_abs_shap, top_features\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gWF1M7bmlXuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "### Now, doing SHAP analysis ###\n",
        "################################\n",
        "\n",
        "# A really good article on SHAP and SHAPley values and how they are calculated - https://medium.com/data-science/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30\n",
        "# SHAP is quantifying the impcat that each feature has on the model's prediction\n",
        "\n",
        "# However, unlike a model which is using specific features in input data, such as age, salary etc, we are using a text query\n",
        "# So, I think it makes sense to\n",
        "# 1. Identify important PCA features using SHAP\n",
        "# 2. Trace them back to embedding dimensions\n",
        "# 3. Then to actual text queries, to give some semantically understandable meaning\n",
        "\n",
        "import shap\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pk\n",
        "from sklearn.decomposition import PCA\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Set some values for analysis\n",
        "no_top_PCA_features = 3\n",
        "no_top_embedding_dimensions = 3\n",
        "no_top_text_queries = 10\n",
        "\n",
        "queries_raw = X_test_raw\n",
        "encoder_name = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "shap_diff = feature_importance_graph(X_train, X_test, model_base)\n",
        "\n",
        "# Look at graoh produced to understand the features which have the biggest SHAP value and thus effect on model\n",
        "# Then want to investigate these features in a semantically meaningful way\n",
        "\n",
        "mean_abs_shap, top_features = get_associated_queries_for_top_features(no_top_PCA_features, no_top_embedding_dimensions, no_top_text_queries, data_pca, encoder_name, queries_raw, shap_diff)\n",
        "\n",
        "# An idea that has occured to me is to use LLMs to try to come up with a semantic value for each top PCA feature, given I have got queries in plain text\n",
        "# Possibly something to explore?"
      ],
      "metadata": {
        "id": "HnYx4iBzntv3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}